import os
import json
import requests
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import google.generativeai as genai

# --- 1. SETUP AND CONFIGURATION ---
# Load environment variables from a .env file
load_dotenv()
try:
    # Configure the Google Generative AI SDK with the API key
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY not found in environment variables.")
    genai.configure(api_key=api_key)
    print("✅ Google AI SDK configured successfully.")
except Exception as e:
    print(f"❌ Error configuring SDK: {e}")
    exit()

# --- 2. PROMPT ENGINEERING ---
# Define the AI's primary persona: an expert PLC programmer
SYSTEM_PROMPT = """You are an expert-level PLC programming assistant specializing in ABB PLC systems. Your primary goal is to generate clean, efficient, and well-documented code in formats like Ladder Diagram (LD), Structured Text (ST), or Instruction List (IL).

Your response MUST be a single JSON object. Do not include any text, explanations, or markdown formatting like ```json before or after the JSON object.

The JSON object must have the following structure:
{
  "title": "A descriptive title for the PLC program.",
  "description": "A brief explanation of what the program does, its inputs, and its outputs.",
  "language": "The PLC programming language used (e.g., 'Structured Text', 'Ladder Diagram').",
  "code": "The complete and syntactically correct PLC code as a single string. Use '\\n' for newlines.",
  "warnings": ["A list of potential issues or important considerations for the user.", "Example: Ensure timer T1 preset is adjusted for motor specifications."]
}
"""

# Define the AI's secondary persona: a syntax and compatibility reviewer
SELF_CORRECTION_PROMPT = """You are a meticulous PLC Syntax and Compatibility Reviewer. You will be given a JSON object that was generated by another AI. Your sole task is to review it and ensure it strictly adheres to all requirements.

1.  **Validate Structure**: The input must be a single, valid JSON object with the keys: "title", "description", "language", "code", and "warnings".
2.  **Verify Code Correctness**: The "code" field must contain syntactically correct and complete PLC code for the specified "language".
3.  **No Extra Text**: Your output must ONLY be the corrected JSON object. Do not add any explanations, apologies, or markdown formatting.

If the input is perfect, return it as-is. If it has errors, correct them and return the fixed JSON object.
"""

# --- 3. FASTAPI APP INITIALIZATION ---
app = FastAPI()

# Configure CORS (Cross-Origin Resource Sharing) to allow web frontends to connect
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins for simplicity; restrict in production
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods (GET, POST, etc.)
    allow_headers=["*"],  # Allows all headers
)

# Define the request data model using Pydantic for automatic validation
class Prompt(BaseModel):
    prompt: str

# --- 4. HELPER FUNCTIONS ---
def call_rag_service(prompt: str) -> list:
    """
    Calls an external RAG service to get context snippets for the prompt.
    """
    # NOTE: 'rag_service' is a container name. For local testing,
    # you might change this to 'localhost' or '127.0.0.1'.
    rag_service_url = "http://rag_service:8001/query-kb"
    try:
        response = requests.post(rag_service_url, json={"prompt": prompt}, timeout=10)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
        return response.json().get("snippets", [])
    except requests.exceptions.RequestException as e:
        print(f"⚠️ WARNING: Could not connect to RAG service at {rag_service_url}: {e}. Proceeding without RAG.")
        return []

def generate_from_llm(user_prompt: str, original_user_input: str, is_correction=False) -> dict:
    """
    Calls the Google Gemini model and handles potential JSON parsing errors.
    """
    try:
        # Select the persona based on whether this is a correction step
        system_instruction = SELF_CORRECTION_PROMPT if is_correction else SYSTEM_PROMPT
        model = genai.GenerativeModel(
            model_name='gemini-1.5-flash',
            system_instruction=system_instruction
        )
        
        # Generate content from the model
        response = model.generate_content(user_prompt)
        
        # Clean the response to ensure it's valid JSON
        cleaned_response = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(cleaned_response)

    except json.JSONDecodeError:
        print("⚠️ Initial JSON parse failed. Raw AI output was not valid JSON.")
        
        # --- CHAT FALLBACK LOGIC ---
        # If the AI returns plain text, check if it was likely a simple chat message
        chat_keywords = ["hi", "hello", "thanks", "thank you", "hey"]
        is_short_prompt = len(original_user_input.split()) <= 3
        is_greeting = any(keyword in original_user_input.lower() for keyword in chat_keywords)

        if is_short_prompt or is_greeting:
            print("...Identified as likely chat. Wrapping raw response in chat JSON.")
            return {"response_type": "chat", "message": response.text.strip()}
        else:
            print(f"❌ ERROR: Failed to parse JSON from a complex prompt.")
            raise HTTPException(status_code=500, detail=f"The AI returned a malformed response: {response.text}")
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred with the AI model: {e}")

# --- 5. MAIN API ENDPOINT ---
@app.post("/generate")
def generate_and_verify_endpoint(prompt: Prompt):
    """
    The main endpoint that orchestrates the RAG, generation, and self-correction process.
    """
    # Step 1: Call RAG service to get context snippets
    rag_snippets = call_rag_service(prompt.prompt)
    rag_context = "\n".join(rag_snippets)

    # Step 2: Construct the initial prompt for the "expert" AI persona
    initial_user_prompt = (
        f"Here is some context from the knowledge base:\n---CONTEXT---\n{rag_context}\n---END CONTEXT---\n\n"
        f"Based on this context and your expertise, please fulfill the following user request: '{prompt.prompt}'"
    )

    # Step 3: Make the first call to the LLM to generate the initial code
    print("➡️ Making initial generation call to LLM...")
    initial_json_response = generate_from_llm(
        user_prompt=initial_user_prompt,
        original_user_input=prompt.prompt,
        is_correction=False  # Use the main SYSTEM_PROMPT
    )

    # Step 4: Construct the prompt for the "reviewer" AI persona
    correction_user_prompt = (
        f"Please review and correct the following JSON output which was generated for the user request: '{prompt.prompt}'. "
        f"Ensure it is syntactically correct and adheres to all best practices.\n\n"
        f"```json\n{json.dumps(initial_json_response, indent=2)}\n```"
    )

    # Step 5: Make the second call to the LLM for self-correction
    print("➡️ Making self-correction call to LLM...")
    final_json = generate_from_llm(
        user_prompt=correction_user_prompt,
        original_user_input=prompt.prompt,
        is_correction=True # Use the SELF_CORRECTION_PROMPT
    )
    
    return {"response_type": "plc_code", "data": final_json}

# --- 6. SERVER EXECUTION ---
if __name__ == "__main__":
    # Run the FastAPI server using uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)